{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mw5OKige3bGc",
        "outputId": "0a9909f7-c32b-4371-9b36-fe37050684c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting langchain_text_splitters\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.12.3)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Collecting langchain-core<2.0.0,>=1.2.0 (from langchain_text_splitters)\n",
            "  Downloading langchain_core-1.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.4.58)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.12.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.25.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.0-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.9/475.9 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m135.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=3a273608376f313fe507f5eaa9cb70df9069faebf35d1e7104da65620a54d770\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pyproject_hooks, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, langchain-core, langchain_text_splitters, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.1.3\n",
            "    Uninstalling langchain-core-1.1.3:\n",
            "      Successfully uninstalled langchain-core-1.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 chromadb-1.3.7 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 langchain-core-1.2.0 langchain_text_splitters-1.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pypika-0.48.9 pyproject_hooks-1.2.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb sentence-transformers transformers torch pandas langchain_text_splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF1eBTd63e_K",
        "outputId": "d5e0ee21-09e7-4993-da1a-624fac683cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aMSn_cY73TI0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "import chromadb\n",
        "import torch\n",
        "from chromadb.utils import embedding_functions\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waaO9rnN3aPF",
        "outputId": "b1c8951f-d8ad-482b-b082-d28f17151e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up ChromaDB...\n"
          ]
        }
      ],
      "source": [
        "# ===== SETUP =====\n",
        "print(\"Setting up ChromaDB...\")\n",
        "client = chromadb.Client()\n",
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "    model_name=\"all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "collection = client.create_collection(\n",
        "    name=\"cooking_assistant\",\n",
        "    embedding_function=sentence_transformer_ef\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this when rerunning the code\n",
        "# client = chromadb.PersistentClient(path=\"/content/drive/MyDrive/final-project/chroma_db\")\n",
        "\n",
        "# Delete and recreate in one go\n",
        "client.delete_collection(name=\"cooking_assistant\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c_I0hM5yQGOb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "JQbt0-Pt4UF0",
        "outputId": "b9ce5b14-93ea-40a1-9347-38f3ce8950e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading language model (this may take a minute)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-612357516.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CPU'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m generator = pipeline(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;34m'text-generation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         self.check_model_type(\n\u001b[1;32m    123\u001b[0m             \u001b[0mTF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, device, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         ):\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;31m# If it's a generation pipeline and the model can generate:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4341\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4342\u001b[0m                 )\n\u001b[0;32m-> 4343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                     )\n\u001b[0;32m-> 1357\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load LLM (only once at startup)\n",
        "print(\"Loading language model (this may take a minute)...\")\n",
        "\n",
        "# Check device availability: CUDA (NVIDIA) > MPS (Apple Silicon) > CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = 0  # CUDA GPU\n",
        "    device_name = 'CUDA GPU'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')  # Apple Silicon GPU\n",
        "    device_name = 'MPS (Apple Silicon GPU)'\n",
        "else:\n",
        "    device = -1  # CPU\n",
        "    device_name = 'CPU'\n",
        "\n",
        "generator = pipeline(\n",
        "    'text-generation',\n",
        "    model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "    device=device\n",
        ")\n",
        "print(f\"Model loaded on: {device_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== NEW CHUNKING FUNCTIONS =====\n",
        "\n",
        "def chunk_fcs_json(data, source_file):\n",
        "    chunks = []\n",
        "\n",
        "    # Introduction\n",
        "    chunks.append({\n",
        "        'text': f\"{data['title']}\\n\\n{data['introduction']}\",\n",
        "        'metadata': {'type': 'introduction', 'source': source_file}\n",
        "    })\n",
        "\n",
        "    # Cooking methods and techniques\n",
        "    for method_name, method_data in data['cooking_methods'].items():\n",
        "        if isinstance(method_data, dict) and 'techniques' in method_data:\n",
        "            for technique_name, technique_desc in method_data['techniques'].items():\n",
        "                chunks.append({\n",
        "                    'text': f\"Cooking Method: {method_name.replace('_', ' ').title()}\\n\\n\"\n",
        "                            f\"{method_data['description']}\\n\\n\"\n",
        "                            f\"Technique: {technique_name}\\n{technique_desc}\",\n",
        "                    'metadata': {\n",
        "                        'type': 'technique',\n",
        "                        'method': method_name,\n",
        "                        'technique': technique_name,\n",
        "                        'source': source_file\n",
        "                    }\n",
        "                })\n",
        "        else:\n",
        "            chunks.append({\n",
        "                'text': f\"Cooking Method: {method_name.title()}\\n\\n{method_data['description']}\",\n",
        "                'metadata': {'type': 'method', 'method': method_name, 'source': source_file}\n",
        "            })\n",
        "\n",
        "    # Kitchen tools\n",
        "    if 'kitchen_tools' in data:\n",
        "        for category, category_data in data['kitchen_tools'].items():\n",
        "            for tool_name, tool_desc in category_data['items'].items():\n",
        "                chunks.append({\n",
        "                    'text': f\"Kitchen Tool: {tool_name}\\n\\n{tool_desc}\",\n",
        "                    'metadata': {\n",
        "                        'type': 'kitchen_tool',\n",
        "                        'category': category,\n",
        "                        'tool': tool_name,\n",
        "                        'source': source_file\n",
        "                    }\n",
        "                })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_healthy_cooking(data, source_file):\n",
        "    chunks = []\n",
        "\n",
        "    for method_name, method_desc in data['cooking_methods'].items():\n",
        "        chunks.append({\n",
        "            'text': f\"Healthy Cooking Method: {method_name}\\n\\n{method_desc}\",\n",
        "            'metadata': {\n",
        "                'type': 'healthy_method',\n",
        "                'method': method_name.lower(),\n",
        "                'source': source_file\n",
        "            }\n",
        "        })\n",
        "\n",
        "    if 'food_preparation_tips' in data:\n",
        "        tips_text = \"Healthy Food Preparation Tips:\\n\\n\" + \"\\n\\n\".join(\n",
        "            f\"• {tip}\" for tip in data['food_preparation_tips']\n",
        "        )\n",
        "        chunks.append({\n",
        "            'text': tips_text,\n",
        "            'metadata': {'type': 'preparation_tips', 'source': source_file}\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_recipe_json(data, source_file):\n",
        "    ingredients_text = \"\\n\".join(\n",
        "        f\"- {ing['item']}: {ing['quantity']}\"\n",
        "        for ing in data['ingredients']\n",
        "    )\n",
        "\n",
        "    recipe_text = f\"\"\"Recipe: {data['dish_name']}\n",
        "\n",
        "Prep time: {data.get('prep_time', 'N/A')}\n",
        "Cooking time: {data.get('cooking_time', 'N/A')}\n",
        "Portions: {data.get('portions', 'N/A')}\n",
        "\n",
        "Ingredients:\n",
        "{ingredients_text}\"\"\"\n",
        "\n",
        "    return [{\n",
        "        'text': recipe_text,\n",
        "        'metadata': {\n",
        "            'type': 'recipe',\n",
        "            'dish_name': data['dish_name'],\n",
        "            'source': source_file\n",
        "        }\n",
        "    }]\n",
        "\n",
        "\n",
        "def chunk_csv_simple(file_path):\n",
        "    \"\"\"\n",
        "    Put entire CSV in one chunk (for small CSVs ~5 lines)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Format as readable text\n",
        "        text_parts = [f\"Data from {file_path.split('/')[-1]}:\\n\"]\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            row_text = \" | \".join([f\"{col}: {val}\" for col, val in row.items() if pd.notna(val)])\n",
        "            text_parts.append(row_text)\n",
        "\n",
        "        text = \"\\n\".join(text_parts)\n",
        "\n",
        "        return [{\n",
        "            'text': text,\n",
        "            'metadata': {\n",
        "                'type': 'csv_data',\n",
        "                'source': file_path,\n",
        "                'filename': file_path.split('/')[-1],\n",
        "                'rows': len(df),\n",
        "                'columns': len(df.columns)\n",
        "            }\n",
        "        }]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error processing CSV {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def chunk_json_simple(file_path):\n",
        "    \"\"\"\n",
        "    Put each row of JSON in its own chunk - subject: text related to subject format\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for key, val in data.items():\n",
        "            # Ensure metadata values are simple types (str, int, float, bool, None)\n",
        "            # Convert dicts or lists to string representation for metadata\n",
        "            metadata_value = json.dumps(val) if isinstance(val, (dict, list)) else val\n",
        "\n",
        "            chunks.append({\n",
        "                'text': f\"{key}: {val}\", # Original value can remain in the text content\n",
        "                'metadata': {\n",
        "                    'type': 'json_data',\n",
        "                    'source': file_path,\n",
        "                    'key': key,\n",
        "                    'value': metadata_value # Store string representation in metadata\n",
        "                }\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error processing JSON {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_reddit_json(file_path):\n",
        "    \"\"\"\n",
        "    Process Reddit JSON with posts and comments grouped together\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        source_name = file_path.split('/')[-1].replace('.json', '')\n",
        "\n",
        "        for post_idx, post in enumerate(data):\n",
        "            # --- Create post chunk ---\n",
        "            post_text = f\"Title: {post['title']}\\n\\nPost: {post['selftext']}\"\n",
        "            chunks.append({\n",
        "                'text': post_text,\n",
        "                'metadata': {\n",
        "                    'type': 'reddit_post',\n",
        "                    'source': file_path,\n",
        "                    'post_title': post['title']\n",
        "                }\n",
        "            })\n",
        "\n",
        "            # --- Create comment chunks (group 3–5 comments each) ---\n",
        "            comments = post.get(\"comments\", [])\n",
        "            group_size = 5\n",
        "\n",
        "            for i in range(0, len(comments), group_size):\n",
        "                group = comments[i:i+group_size]\n",
        "                comment_text = f\"Comments for post '{post['title']}':\\n\"\n",
        "\n",
        "                for c in group:\n",
        "                    comment_text += f\"{c['author']}: {c['body']}\\n\"\n",
        "\n",
        "                chunks.append({\n",
        "                    'text': comment_text,\n",
        "                    'metadata': {\n",
        "                        'type': 'reddit_comments',\n",
        "                        'source': file_path,\n",
        "                        'post_title': post['title']\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        print(f\"✓ Loaded {len(chunks)} chunks from Reddit data: {source_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error processing Reddit JSON {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def apply_recursive_chunking(chunks, chunk_size=500, chunk_overlap=50):\n",
        "    \"\"\"\n",
        "    Apply RecursiveCharacterTextSplitter to long chunks\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"]\n",
        "    )\n",
        "\n",
        "    final_chunks = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        text_parts = splitter.split_text(chunk['text'])\n",
        "\n",
        "        # If text was split into multiple parts\n",
        "        if len(text_parts) > 1:\n",
        "            for i, part in enumerate(text_parts):\n",
        "                final_chunks.append({\n",
        "                    'text': part,\n",
        "                    'metadata': {**chunk['metadata'], 'chunk_part': i}\n",
        "                })\n",
        "        else:\n",
        "            # Keep original chunk if it's already small enough\n",
        "            final_chunks.append(chunk)\n",
        "\n",
        "    return final_chunks\n",
        "\n",
        "\n",
        "def chunk_ingredient_data_json(data, source_file):\n",
        "    \"\"\"\n",
        "    Chunk ingredient pairing data from the flavor network info\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # --- 1. Individual ingredient pair chunks ---\n",
        "    # Each pair becomes its own searchable chunk\n",
        "    if 'ingredient_pairs' in data:\n",
        "        for pair in data['ingredient_pairs']:\n",
        "            ing1 = pair['ingredient1'].replace('_', ' ').title()\n",
        "            ing2 = pair['ingredient2'].replace('_', ' ').title()\n",
        "            shared = pair['num_shared_compound']\n",
        "\n",
        "            # Create descriptive text for the pairing\n",
        "            pairing_strength = \"strongly\" if shared > 30 else \"moderately\" if shared > 15 else \"somewhat\"\n",
        "\n",
        "            text = f\"\"\"Ingredient Pairing: {ing1} and {ing2}\n",
        "\n",
        "These ingredients share {shared} chemical compounds, suggesting they {pairing_strength} complement each other in recipes.\n",
        "\n",
        "Pairing strength: {pairing_strength.title()}\n",
        "Shared compounds: {shared}\"\"\"\n",
        "\n",
        "            chunks.append({\n",
        "                'text': text,\n",
        "                'metadata': {\n",
        "                    'type': 'ingredient_pairing',\n",
        "                    'ingredient1': pair['ingredient1'],\n",
        "                    'ingredient2': pair['ingredient2'],\n",
        "                    'shared_compounds': shared,\n",
        "                    'pairing_strength': pairing_strength,\n",
        "                    'source': source_file\n",
        "                }\n",
        "            })\n",
        "\n",
        "    # --- 2. Grouped pairing chunks by ingredient ---\n",
        "    # For queries like \"what pairs well with orange?\"\n",
        "    if 'ingredient_pairs' in data:\n",
        "        # Build a lookup: ingredient -> list of pairings\n",
        "        ingredient_map = {}\n",
        "\n",
        "        for pair in data['ingredient_pairs']:\n",
        "            ing1 = pair['ingredient1']\n",
        "            ing2 = pair['ingredient2']\n",
        "            shared = pair['num_shared_compound']\n",
        "\n",
        "            # Add to ing1's pairing list\n",
        "            if ing1 not in ingredient_map:\n",
        "                ingredient_map[ing1] = []\n",
        "            ingredient_map[ing1].append((ing2, shared))\n",
        "\n",
        "            # Add to ing2's pairing list (bidirectional)\n",
        "            if ing2 not in ingredient_map:\n",
        "                ingredient_map[ing2] = []\n",
        "            ingredient_map[ing2].append((ing1, shared))\n",
        "\n",
        "        # Create a chunk for each ingredient's pairings\n",
        "        for ingredient, pairings in ingredient_map.items():\n",
        "            # Sort by shared compounds (best pairings first)\n",
        "            pairings.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            ing_name = ingredient.replace('_', ' ').title()\n",
        "\n",
        "            # Build pairing list text\n",
        "            pairing_lines = []\n",
        "            for paired_ing, shared in pairings[:10]:  # Top 10 pairings\n",
        "                paired_name = paired_ing.replace('_', ' ').title()\n",
        "                pairing_lines.append(f\"  • {paired_name} ({shared} shared compounds)\")\n",
        "\n",
        "            text = f\"\"\"Ingredient Pairings for {ing_name}\n",
        "\n",
        "Based on shared chemical compounds, {ing_name} pairs well with:\n",
        "\n",
        "{chr(10).join(pairing_lines)}\n",
        "\n",
        "These pairings are ranked by the number of shared flavor compounds.\"\"\"\n",
        "\n",
        "            chunks.append({\n",
        "                'text': text,\n",
        "                'metadata': {\n",
        "                    'type': 'ingredient_pairing_summary',\n",
        "                    'ingredient': ingredient,\n",
        "                    'num_pairings': len(pairings),\n",
        "                    'best_pairing': pairings[0][0] if pairings else None,\n",
        "                    'best_pairing_compounds': pairings[0][1] if pairings else 0,\n",
        "                    'source': source_file\n",
        "                }\n",
        "            })\n",
        "\n",
        "    # --- 3. Individual ingredient info chunks ---\n",
        "    if 'ingredients' in data:\n",
        "        for ingredient, info in data['ingredients'].items():\n",
        "            ing_name = ingredient.replace('_', ' ').title()\n",
        "            category = info.get('category', 'unknown').title()\n",
        "            prevalence = info.get('prevalence', 0)\n",
        "\n",
        "            text = f\"\"\"Ingredient: {ing_name}\n",
        "\n",
        "Category: {category}\n",
        "Prevalence in recipes: {prevalence:.2%}\n",
        "\n",
        "This ingredient appears in approximately {prevalence:.1%} of recipes in the database.\"\"\"\n",
        "\n",
        "            chunks.append({\n",
        "                'text': text,\n",
        "                'metadata': {\n",
        "                    'type': 'ingredient_info',\n",
        "                    'ingredient': ingredient,\n",
        "                    'category': info.get('category'),\n",
        "                    'prevalence': prevalence,\n",
        "                    'source': source_file\n",
        "                }\n",
        "            })\n",
        "\n",
        "    # --- 4. Category-based pairing chunks (optional) ---\n",
        "    # Group pairings by category combinations\n",
        "    if 'ingredient_pairs' in data and 'ingredients' in data:\n",
        "        category_pairings = {}\n",
        "\n",
        "        for pair in data['ingredient_pairs']:\n",
        "            ing1 = pair['ingredient1']\n",
        "            ing2 = pair['ingredient2']\n",
        "\n",
        "            # Get categories\n",
        "            cat1 = data['ingredients'].get(ing1, {}).get('category', 'unknown')\n",
        "            cat2 = data['ingredients'].get(ing2, {}).get('category', 'unknown')\n",
        "\n",
        "            # Create category pair key\n",
        "            cat_key = tuple(sorted([cat1, cat2]))\n",
        "\n",
        "            if cat_key not in category_pairings:\n",
        "                category_pairings[cat_key] = []\n",
        "\n",
        "            category_pairings[cat_key].append({\n",
        "                'ing1': ing1.replace('_', ' ').title(),\n",
        "                'ing2': ing2.replace('_', ' ').title(),\n",
        "                'shared': pair['num_shared_compound']\n",
        "            })\n",
        "\n",
        "        # Create chunks for each category pairing\n",
        "        for (cat1, cat2), pairings in category_pairings.items():\n",
        "            if len(pairings) < 3:  # Skip categories with too few pairings\n",
        "                continue\n",
        "\n",
        "            # Get top 5 examples\n",
        "            pairings.sort(key=lambda x: x['shared'], reverse=True)\n",
        "            examples = pairings[:5]\n",
        "\n",
        "            example_lines = [f\"  • {p['ing1']} + {p['ing2']} ({p['shared']} compounds)\"\n",
        "                           for p in examples]\n",
        "\n",
        "            text = f\"\"\"Category Pairing: {cat1.title()} with {cat2.title()}\n",
        "\n",
        "Ingredients from these categories often pair well together. Top examples:\n",
        "\n",
        "{chr(10).join(example_lines)}\n",
        "\n",
        "Total pairings found: {len(pairings)}\"\"\"\n",
        "\n",
        "            chunks.append({\n",
        "                'text': text,\n",
        "                'metadata': {\n",
        "                    'type': 'category_pairing',\n",
        "                    'category1': cat1,\n",
        "                    'category2': cat2,\n",
        "                    'num_pairings': len(pairings),\n",
        "                    'source': source_file\n",
        "                }\n",
        "            })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def chunk_cuisine_ingredients_dict(cuisine_dict, source_file):\n",
        "    \"\"\"\n",
        "    Chunk pre-parsed cuisine-ingredient dictionary\n",
        "    Creates multiple chunk types for different retrieval patterns\n",
        "\n",
        "    Args:\n",
        "        cuisine_dict: Dictionary with cuisine as key, list of ingredient lists as value\n",
        "        source_file: Path to source file for metadata\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    try:\n",
        "        # --- 1. Individual recipe chunks ---\n",
        "        # Each recipe as a standalone chunk\n",
        "        recipe_idx = 0\n",
        "        for cuisine, recipe_list in cuisine_dict.items():\n",
        "            for ingredients in recipe_list:\n",
        "                ing_formatted = ', '.join([ing.replace('_', ' ').title() for ing in ingredients])\n",
        "\n",
        "                text = f\"\"\"Cuisine: {cuisine}\n",
        "\n",
        "Traditional {cuisine} dish using: {ing_formatted}\n",
        "\n",
        "Number of ingredients: {len(ingredients)}\"\"\"\n",
        "\n",
        "                chunks.append({\n",
        "                    'text': text,\n",
        "                    'metadata': {\n",
        "                        'type': 'cuisine_recipe',\n",
        "                        'cuisine': cuisine.lower(),\n",
        "                        'num_ingredients': len(ingredients),\n",
        "                        'ingredients': '|'.join(ingredients),  # Store as pipe-separated string\n",
        "                        'source': source_file\n",
        "                    }\n",
        "                })\n",
        "                recipe_idx += 1\n",
        "\n",
        "        # --- 2. Cuisine summary chunks ---\n",
        "        # Aggregated view per cuisine showing common ingredients\n",
        "        for cuisine, recipe_list in cuisine_dict.items():\n",
        "            # Count ingredient frequency across all recipes in this cuisine\n",
        "            from collections import defaultdict\n",
        "            ingredient_freq = defaultdict(int)\n",
        "            for recipe in recipe_list:\n",
        "                for ing in recipe:\n",
        "                    ingredient_freq[ing] += 1\n",
        "\n",
        "            # Sort by frequency\n",
        "            sorted_ingredients = sorted(ingredient_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Get top 10 most common ingredients\n",
        "            top_ingredients = sorted_ingredients[:10]\n",
        "            ing_lines = [f\"  • {ing.replace('_', ' ').title()} (used in {count}/{len(recipe_list)} recipes)\"\n",
        "                        for ing, count in top_ingredients]\n",
        "\n",
        "            text = f\"\"\"{cuisine} Cuisine Profile\n",
        "\n",
        "Based on {len(recipe_list)} traditional recipes, common ingredients include:\n",
        "\n",
        "{chr(10).join(ing_lines)}\n",
        "\n",
        "This cuisine features {len(ingredient_freq)} unique ingredients across its dishes.\"\"\"\n",
        "\n",
        "            chunks.append({\n",
        "                'text': text,\n",
        "                'metadata': {\n",
        "                    'type': 'cuisine_summary',\n",
        "                    'cuisine': cuisine.lower(),\n",
        "                    'num_recipes': len(recipe_list),\n",
        "                    'num_unique_ingredients': len(ingredient_freq),\n",
        "                    'most_common_ingredient': top_ingredients[0][0] if top_ingredients else None,\n",
        "                    'source': source_file\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # --- 3. Ingredient-to-cuisine mapping chunks ---\n",
        "        # For queries like \"what cuisines use ginger?\"\n",
        "        from collections import defaultdict\n",
        "        ingredient_to_cuisines = defaultdict(set)\n",
        "\n",
        "        for cuisine, recipe_list in cuisine_dict.items():\n",
        "            for recipe in recipe_list:\n",
        "                for ing in recipe:\n",
        "                    ingredient_to_cuisines[ing].add(cuisine)\n",
        "\n",
        "        # Create chunks for ingredients used in multiple cuisines\n",
        "        for ingredient, cuisines in ingredient_to_cuisines.items():\n",
        "            if len(cuisines) >= 2:  # Only include if used in 2+ cuisines\n",
        "                ing_name = ingredient.replace('_', ' ').title()\n",
        "                cuisine_list = ', '.join(sorted(cuisines))\n",
        "\n",
        "                text = f\"\"\"Ingredient: {ing_name}\n",
        "\n",
        "Found in {len(cuisines)} cuisines: {cuisine_list}\n",
        "\n",
        "This ingredient is versatile and appears across multiple culinary traditions.\"\"\"\n",
        "\n",
        "                chunks.append({\n",
        "                    'text': text,\n",
        "                    'metadata': {\n",
        "                        'type': 'ingredient_cuisine_map',\n",
        "                        'ingredient': ingredient,\n",
        "                        'num_cuisines': len(cuisines),\n",
        "                        'cuisines': '|'.join(sorted(cuisines)),\n",
        "                        'source': source_file\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        # --- 4. Recipe similarity chunks ---\n",
        "        # Group recipes by ingredient overlap for substitution suggestions\n",
        "        for cuisine, recipe_list in cuisine_dict.items():\n",
        "            # Find recipes with high ingredient overlap (5+ shared ingredients)\n",
        "            for i, recipe1 in enumerate(recipe_list):\n",
        "                for j in range(i+1, len(recipe_list)):\n",
        "                    recipe2 = recipe_list[j]\n",
        "                    shared = set(recipe1) & set(recipe2)\n",
        "\n",
        "                    if len(shared) >= 5:\n",
        "                        shared_formatted = ', '.join([ing.replace('_', ' ').title() for ing in sorted(shared)])\n",
        "\n",
        "                        text = f\"\"\"{cuisine} Recipe Pairing\n",
        "\n",
        "Two traditional {cuisine} dishes share these ingredients: {shared_formatted}\n",
        "\n",
        "Shared ingredients: {len(shared)}\n",
        "\n",
        "These recipes could be good to prepare together as they use similar ingredients.\"\"\"\n",
        "\n",
        "                        chunks.append({\n",
        "                            'text': text,\n",
        "                            'metadata': {\n",
        "                                'type': 'recipe_similarity',\n",
        "                                'cuisine': cuisine.lower(),\n",
        "                                'shared_count': len(shared),\n",
        "                                'source': source_file\n",
        "                            }\n",
        "                        })\n",
        "\n",
        "        print(f\"✓ Created {len(chunks)} cuisine-ingredient chunks ({recipe_idx} recipes)\")\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error processing cuisine data: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def chunk_cuisine_ingredients_dict(cuisine_dict, source_file):\n",
        "    \"\"\"\n",
        "    Chunk pre-parsed cuisine-ingredient dictionary\n",
        "    Creates multiple chunk types for different retrieval patterns\n",
        "\n",
        "    Args:\n",
        "        cuisine_dict: Dictionary with cuisine as key, list of ingredient lists as value\n",
        "        source_file: Path to source file for metadata\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    try:\n",
        "        # --- 1. Individual recipe chunks ---\n",
        "        # Each recipe as a standalone chunk\n",
        "        recipe_idx = 0\n",
        "        for cuisine, recipe_list in cuisine_dict.items():\n",
        "            for ingredients in recipe_list:\n",
        "                ing_formatted = ', '.join([ing.replace('_', ' ').title() for ing in ingredients])\n",
        "\n",
        "                text = f\"\"\"Cuisine: {cuisine}\n",
        "\n",
        "Traditional {cuisine} dish using: {ing_formatted}\n",
        "\n",
        "Number of ingredients: {len(ingredients)}\"\"\"\n",
        "\n",
        "                chunks.append({\n",
        "                    'text': text,\n",
        "                    'metadata': {\n",
        "                        'type': 'cuisine_recipe',\n",
        "                        'cuisine': cuisine.lower(),\n",
        "                        'num_ingredients': len(ingredients),\n",
        "                        'ingredients': '|'.join(ingredients),  # Store as pipe-separated string\n",
        "                        'source': source_file\n",
        "                    }\n",
        "                })\n",
        "                recipe_idx += 1\n",
        "\n",
        "        # --- 2. Cuisine summary chunks ---\n",
        "        # Aggregated view per cuisine showing common ingredients\n",
        "        for cuisine, recipe_list in cuisine_dict.items():\n",
        "            # Count ingredient frequency across all recipes in this cuisine\n",
        "            from collections import defaultdict\n",
        "            ingredient_freq = defaultdict(int)\n",
        "            for recipe in recipe_list:\n",
        "                for ing in recipe:\n",
        "                    ingredient_freq[ing] += 1\n",
        "\n",
        "            # Sort by frequency\n",
        "            sorted_ingredients = sorted(ingredient_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Get top 10 most common ingredients\n",
        "            top_ingredients = sorted_ingredients[:10]\n",
        "            ing_lines = [f\"  • {ing.replace('_', ' ').title()} (used in {count}/{len(recipe_list)} recipes)\"\n",
        "                        for ing, count in top_ingredients]\n",
        "\n",
        "            text = f\"\"\"{cuisine} Cuisine Profile\n",
        "\n",
        "Based on {len(recipe_list)} traditional recipes, common ingredients include:\n",
        "\n",
        "{chr(10).join(ing_lines)}\n",
        "\n",
        "This cuisine features {len(ingredient_freq)} unique ingredients across its dishes.\"\"\"\n",
        "\n",
        "            chunks.append({\n",
        "                'text': text,\n",
        "                'metadata': {\n",
        "                    'type': 'cuisine_summary',\n",
        "                    'cuisine': cuisine.lower(),\n",
        "                    'num_recipes': len(recipe_list),\n",
        "                    'num_unique_ingredients': len(ingredient_freq),\n",
        "                    'most_common_ingredient': top_ingredients[0][0] if top_ingredients else None,\n",
        "                    'source': source_file\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # --- 3. Ingredient-to-cuisine mapping chunks ---\n",
        "        # For queries like \"what cuisines use ginger?\"\n",
        "        from collections import defaultdict\n",
        "        ingredient_to_cuisines = defaultdict(set)\n",
        "\n",
        "        for cuisine, recipe_list in cuisine_dict.items():\n",
        "            for recipe in recipe_list:\n",
        "                for ing in recipe:\n",
        "                    ingredient_to_cuisines[ing].add(cuisine)\n",
        "\n",
        "        # Create chunks for ingredients used in multiple cuisines\n",
        "        for ingredient, cuisines in ingredient_to_cuisines.items():\n",
        "            if len(cuisines) >= 2:  # Only include if used in 2+ cuisines\n",
        "                ing_name = ingredient.replace('_', ' ').title()\n",
        "                cuisine_list = ', '.join(sorted(cuisines))\n",
        "\n",
        "                text = f\"\"\"Ingredient: {ing_name}\n",
        "\n",
        "Found in {len(cuisines)} cuisines: {cuisine_list}\n",
        "\n",
        "This ingredient is versatile and appears across multiple culinary traditions.\"\"\"\n",
        "\n",
        "                chunks.append({\n",
        "                    'text': text,\n",
        "                    'metadata': {\n",
        "                        'type': 'ingredient_cuisine_map',\n",
        "                        'ingredient': ingredient,\n",
        "                        'num_cuisines': len(cuisines),\n",
        "                        'cuisines': '|'.join(sorted(cuisines)),\n",
        "                        'source': source_file\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        # --- 4. Recipe similarity chunks ---\n",
        "        # Group recipes by ingredient overlap for substitution suggestions\n",
        "        for cuisine, recipe_list in cuisine_dict.items():\n",
        "            # Find recipes with high ingredient overlap (5+ shared ingredients)\n",
        "            for i, recipe1 in enumerate(recipe_list):\n",
        "                for j in range(i+1, len(recipe_list)):\n",
        "                    recipe2 = recipe_list[j]\n",
        "                    shared = set(recipe1) & set(recipe2)\n",
        "\n",
        "                    if len(shared) >= 5:\n",
        "                        shared_formatted = ', '.join([ing.replace('_', ' ').title() for ing in sorted(shared)])\n",
        "\n",
        "                        text = f\"\"\"{cuisine} Recipe Pairing\n",
        "\n",
        "Two traditional {cuisine} dishes share these ingredients: {shared_formatted}\n",
        "\n",
        "Shared ingredients: {len(shared)}\n",
        "\n",
        "These recipes could be good to prepare together as they use similar ingredients.\"\"\"\n",
        "\n",
        "                        chunks.append({\n",
        "                            'text': text,\n",
        "                            'metadata': {\n",
        "                                'type': 'recipe_similarity',\n",
        "                                'cuisine': cuisine.lower(),\n",
        "                                'shared_count': len(shared),\n",
        "                                'source': source_file\n",
        "                            }\n",
        "                        })\n",
        "\n",
        "        print(f\"✓ Created {len(chunks)} cuisine-ingredient chunks ({recipe_idx} recipes)\")\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error processing cuisine data: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "RfDDTfuuhAN9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== NEW INGESTION =====\n",
        "\n",
        "def ingest_all_documents(collection, apply_chunking=False):\n",
        "    \"\"\"\n",
        "    Main ingestion function that processes all document types\n",
        "\n",
        "    Args:\n",
        "        collection: ChromaDB collection object\n",
        "        apply_chunking: If True, applies RecursiveCharacterTextSplitter to all chunks\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    # Processed data path\n",
        "    processed_data_folder = '/content/drive/MyDrive/final-project/data/processed'\n",
        "\n",
        "    # Load core technique files\n",
        "    print(\"Loading core cooking technique files...\")\n",
        "    specific_files = {\n",
        "        os.path.join(processed_data_folder, 'FCS.json'): chunk_fcs_json,\n",
        "        os.path.join(processed_data_folder, 'healthy_cooking_method.json'): chunk_healthy_cooking,\n",
        "    }\n",
        "\n",
        "    for file_path, chunk_function in specific_files.items():\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            chunks = chunk_function(data, file_path)\n",
        "            all_chunks.extend(chunks)\n",
        "            print(f\"✓ Loaded {len(chunks)} chunks from {os.path.basename(file_path)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error loading {file_path}: {e}\")\n",
        "\n",
        "    # Load all recipes from foc folder\n",
        "    print(\"\\nLoading recipes from foc folder...\")\n",
        "    foc_folder = os.path.join(processed_data_folder, \"foc\")\n",
        "    recipe_count = 0\n",
        "\n",
        "    if os.path.exists(foc_folder):\n",
        "        foc_json_files = glob.glob(os.path.join(foc_folder, '*.json'))\n",
        "\n",
        "        for file_path in foc_json_files:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                try:\n",
        "                    chunks = chunk_recipe_json(data, file_path)\n",
        "                    all_chunks.extend(chunks)\n",
        "                    recipe_count += 1\n",
        "                except:\n",
        "                    chunks = chunk_json_simple(file_path)\n",
        "                    all_chunks.extend(chunks)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ Error loading {os.path.basename(file_path)}: {e}\")\n",
        "\n",
        "        print(f\"✓ Loaded {recipe_count} recipes\")\n",
        "    else:\n",
        "        print(f\"✗ foc folder not found at {foc_folder}\")\n",
        "\n",
        "    # Load all CSV files (one chunk per file)\n",
        "    print(\"\\nLoading CSV files...\")\n",
        "    csv_files = glob.glob(os.path.join(processed_data_folder, '*.csv'))\n",
        "    csv_count = 0\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        csv_chunks = chunk_csv_simple(csv_file)\n",
        "        if csv_chunks:\n",
        "            all_chunks.extend(csv_chunks)\n",
        "            csv_count += 1\n",
        "            print(f\"✓ Loaded {os.path.basename(csv_file)}\")\n",
        "\n",
        "    # Load ingredient info from flavor network\n",
        "    print(\"\\nLoading ingredient pairing data...\")\n",
        "    pairing_file = os.path.join(processed_data_folder, 'ingredient_data.json')\n",
        "\n",
        "    try:\n",
        "        with open(pairing_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        pairing_chunks = chunk_ingredient_data_json(data, pairing_file)\n",
        "        all_chunks.extend(pairing_chunks)\n",
        "        print(f\"✓ Loaded {len(pairing_chunks)} ingredient pairing chunks\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading ingredient data: {e}\")\n",
        "\n",
        "    # too much memory -- maybe i can try to get it to be shorter\n",
        "    # # Load cuisine-ingredient data\n",
        "    # print(\"\\nLoading cuisine-ingredient data...\")\n",
        "    # cuisine_json_path = os.path.join(processed_data_folder, \"cuisine_ingredient_data.json\")\n",
        "\n",
        "    # with open(cuisine_json_path, 'r', encoding='utf-8') as f:\n",
        "    #     cuisine_json = json.load(f)\n",
        "\n",
        "    #     # Chunk the parsed data\n",
        "    #     cuisine_chunks = chunk_cuisine_ingredients_dict(cuisine_json, cuisine_json_path)\n",
        "    #     all_chunks.extend(cuisine_chunks)\n",
        "    #     print(f\"✓ Loaded {len(cuisine_chunks)} cuisine data chunks\")\n",
        "\n",
        "\n",
        "    # Load Reddit JSON files\n",
        "    print(\"\\nLoading Reddit JSON files...\")\n",
        "    reddit_folder = os.path.join(processed_data_folder, 'reddit')\n",
        "    reddit_files = glob.glob(os.path.join(reddit_folder, '*.json'))\n",
        "    reddit_count = 0\n",
        "\n",
        "    for reddit_file in reddit_files:\n",
        "        reddit_chunks = chunk_reddit_json(reddit_file)\n",
        "        if reddit_chunks:\n",
        "            all_chunks.extend(reddit_chunks)\n",
        "            reddit_count += len(reddit_chunks)\n",
        "\n",
        "    # Apply recursive chunking if requested\n",
        "    if apply_chunking:\n",
        "        print(f\"\\nApplying recursive text splitting...\")\n",
        "        original_count = len(all_chunks)\n",
        "        all_chunks = apply_recursive_chunking(all_chunks)\n",
        "        print(f\"✓ Chunks after splitting: {len(all_chunks)} (was {original_count})\")\n",
        "\n",
        "\n",
        "    # Add all chunks to ChromaDB\n",
        "    if all_chunks:\n",
        "        print(f\"\\nAdding {len(all_chunks)} chunks to ChromaDB...\")\n",
        "\n",
        "        documents = [chunk['text'] for chunk in all_chunks]\n",
        "        metadatas = [chunk['metadata'] for chunk in all_chunks]\n",
        "        ids = [f\"doc_{i}\" for i in range(len(all_chunks))]\n",
        "\n",
        "        # Single call - much simpler\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "        print(\"✓ All documents added successfully!\")\n",
        "\n",
        "    return len(all_chunks)"
      ],
      "metadata": {
        "id": "cTQhckkmjFOb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "RlVdlBEl4deN"
      },
      "outputs": [],
      "source": [
        "# ===== QUERY & GENERATE =====\n",
        "\n",
        "def query_rag(question, n_results=3):\n",
        "    \"\"\"Retrieve relevant chunks\"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=n_results\n",
        "    )\n",
        "    return results['documents'][0], results['metadatas'][0]\n",
        "\n",
        "\n",
        "def ask_cooking_assistant(question):\n",
        "    \"\"\"Main RAG function with Hugging Face\"\"\"\n",
        "    # Retrieve relevant docs\n",
        "    contexts, metadatas = query_rag(question, n_results=3)\n",
        "    context_text = \"\\n\\n---\\n\\n\".join(contexts)\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are a helpful cooking assistant. Answer questions based on the provided context. Be concise and accurate.<|end|>\n",
        "<|user|>\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "Question: {question}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=300,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        return_full_text=False\n",
        "    )\n",
        "\n",
        "    answer = result[0]['generated_text'].strip()\n",
        "    sources = list(set([m['source'].split('/')[-1] for m in metadatas]))\n",
        "\n",
        "    return answer, sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAF4hGrA4gEd",
        "outputId": "f9b25033-1068-456f-8a1c-9f20ea512a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "COOKING ASSISTANT RAG SYSTEM - HUGGING FACE\n",
            "============================================================\n",
            "\n",
            "Step 1: Ingesting documents...\n",
            "Loading core cooking technique files...\n",
            "✓ Loaded 32 chunks from FCS.json\n",
            "✓ Loaded 15 chunks from healthy_cooking_method.json\n",
            "\n",
            "Loading recipes from foc folder...\n",
            "✓ Loaded 33 recipes\n",
            "\n",
            "Loading CSV files...\n",
            "✓ Loaded cooking_methods.csv\n",
            "\n",
            "Loading ingredient pairing data...\n",
            "✓ Loaded 1171 ingredient pairing chunks\n",
            "\n",
            "Loading cuisine-ingredient data...\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Ingest all documents\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COOKING ASSISTANT RAG SYSTEM - HUGGING FACE\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(\"Step 1: Ingesting documents...\")\n",
        "    total_chunks = ingest_all_documents(collection)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"System Ready! Ask cooking questions.\")\n",
        "    print(\"=\"*60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Example queries\n",
        "    test_questions = [\n",
        "        \"What is truffle? what tools can I use with the ingredient?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        answer, sources = ask_cooking_assistant(question)\n",
        "\n",
        "        print(f\"\\nA: {answer}\")\n",
        "        print(f\"\\n📚 Sources: {', '.join(sources)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjbkxRC5kfAh",
        "outputId": "6a397382-2225-4165-9a43-da325a905734"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Q: What is truffle? what tools can I use with the ingredient?\n",
            "============================================================\n",
            "\n",
            "A: The given text mentions a hand-held device called an immersion blender that can quickly blend a smoothie, a soup, a sauce, or a salad dressing. It is not mentioned whether the given text also includes a single-serve blender, but it's implied by the context that such a tool would be used for preparing one serving of a smoothie or shake.\n",
            "\n",
            "📚 Sources: FCS.json, AskBaking_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nk-h1TVHR4N",
        "outputId": "262e2303-51c7-4d86-9545-88e670807e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "A: Answer: Espagnole sauce, which is a cold emulsion sauce made using carrot, beef stock, chopped onion, tomato puree, garlic, celery, butter, black pepper, flour, bay leaves, and prepared in a pan over low heat for 15 minutes until thickened.\n",
            "\n",
            "📚 Sources: foc_ingredients_Espagnole_sauce.json, foc_ingredients_Oriental_sauce.json, foc_sections.json\n"
          ]
        }
      ],
      "source": [
        "question = \"what kind of sauce is make with emulsion?\"\n",
        "\n",
        "answer, sources = ask_cooking_assistant(question)\n",
        "\n",
        "print(f\"\\nA: {answer}\")\n",
        "print(f\"\\n📚 Sources: {', '.join(sources)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}