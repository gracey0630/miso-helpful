{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb sentence-transformers transformers torch pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mw5OKige3bGc",
        "outputId": "6e7a2a45-2ce4-4600-b294-80293500ccff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.3.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.12.3)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.39.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.3.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=52229c04dff06559e51d0a837cad4109fbb0b51d21b76f020eb8744858fc7fa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pyproject_hooks, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 chromadb-1.3.6 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.39.0 opentelemetry-exporter-otlp-proto-common-1.39.0 opentelemetry-exporter-otlp-proto-grpc-1.39.0 opentelemetry-proto-1.39.0 opentelemetry-sdk-1.39.0 opentelemetry-semantic-conventions-0.60b0 posthog-5.4.0 pybase64-1.4.3 pypika-0.48.9 pyproject_hooks-1.2.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF1eBTd63e_K",
        "outputId": "6f181a8c-469b-4d5c-db7f-ea0c804808bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aMSn_cY73TI0"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SETUP =====\n",
        "print(\"Setting up ChromaDB...\")\n",
        "client = chromadb.Client()\n",
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "    model_name=\"all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "collection = client.create_collection(\n",
        "    name=\"cooking_docs\",\n",
        "    embedding_function=sentence_transformer_ef\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waaO9rnN3aPF",
        "outputId": "c9ef7e91-e257-4e56-849f-83bd1a12971a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up ChromaDB...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLM (only once at startup)\n",
        "print(\"Loading language model (this may take a minute)...\")\n",
        "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        "generator = pipeline(\n",
        "    'text-generation',\n",
        "    model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "    device=device\n",
        ")\n",
        "print(f\"Model loaded on: {'GPU' if device == 0 else 'CPU'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQbt0-Pt4UF0",
        "outputId": "68db5c80-15f9-454e-8ce8-0b3d674ffe92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading language model (this may take a minute)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on: CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CHUNKING FUNCTIONS =====\n",
        "\n",
        "def chunk_fcs_json(data, source_file):\n",
        "    chunks = []\n",
        "\n",
        "    # Introduction\n",
        "    chunks.append({\n",
        "        'text': f\"{data['title']}\\n\\n{data['introduction']}\",\n",
        "        'metadata': {'type': 'introduction', 'source': source_file}\n",
        "    })\n",
        "\n",
        "    # Cooking methods and techniques\n",
        "    for method_name, method_data in data['cooking_methods'].items():\n",
        "        if isinstance(method_data, dict) and 'techniques' in method_data:\n",
        "            for technique_name, technique_desc in method_data['techniques'].items():\n",
        "                chunks.append({\n",
        "                    'text': f\"Cooking Method: {method_name.replace('_', ' ').title()}\\n\\n\"\n",
        "                            f\"{method_data['description']}\\n\\n\"\n",
        "                            f\"Technique: {technique_name}\\n{technique_desc}\",\n",
        "                    'metadata': {\n",
        "                        'type': 'technique',\n",
        "                        'method': method_name,\n",
        "                        'technique': technique_name,\n",
        "                        'source': source_file\n",
        "                    }\n",
        "                })\n",
        "        else:\n",
        "            chunks.append({\n",
        "                'text': f\"Cooking Method: {method_name.title()}\\n\\n{method_data['description']}\",\n",
        "                'metadata': {'type': 'method', 'method': method_name, 'source': source_file}\n",
        "            })\n",
        "\n",
        "    # Kitchen tools\n",
        "    if 'kitchen_tools' in data:\n",
        "        for category, category_data in data['kitchen_tools'].items():\n",
        "            for tool_name, tool_desc in category_data['items'].items():\n",
        "                chunks.append({\n",
        "                    'text': f\"Kitchen Tool: {tool_name}\\n\\n{tool_desc}\",\n",
        "                    'metadata': {\n",
        "                        'type': 'kitchen_tool',\n",
        "                        'category': category,\n",
        "                        'tool': tool_name,\n",
        "                        'source': source_file\n",
        "                    }\n",
        "                })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_healthy_cooking(data, source_file):\n",
        "    chunks = []\n",
        "\n",
        "    for method_name, method_desc in data['cooking_methods'].items():\n",
        "        chunks.append({\n",
        "            'text': f\"Healthy Cooking Method: {method_name}\\n\\n{method_desc}\",\n",
        "            'metadata': {\n",
        "                'type': 'healthy_method',\n",
        "                'method': method_name.lower(),\n",
        "                'source': source_file\n",
        "            }\n",
        "        })\n",
        "\n",
        "    if 'food_preparation_tips' in data:\n",
        "        tips_text = \"Healthy Food Preparation Tips:\\n\\n\" + \"\\n\\n\".join(\n",
        "            f\"â€¢ {tip}\" for tip in data['food_preparation_tips']\n",
        "        )\n",
        "        chunks.append({\n",
        "            'text': tips_text,\n",
        "            'metadata': {'type': 'preparation_tips', 'source': source_file}\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_recipe_json(data, source_file):\n",
        "    ingredients_text = \"\\n\".join(\n",
        "        f\"- {ing['item']}: {ing['quantity']}\"\n",
        "        for ing in data['ingredients']\n",
        "    )\n",
        "\n",
        "    recipe_text = f\"\"\"Recipe: {data['dish_name']}\n",
        "\n",
        "Prep time: {data.get('prep_time', 'N/A')}\n",
        "Cooking time: {data.get('cooking_time', 'N/A')}\n",
        "Portions: {data.get('portions', 'N/A')}\n",
        "\n",
        "Ingredients:\n",
        "{ingredients_text}\"\"\"\n",
        "\n",
        "    return [{\n",
        "        'text': recipe_text,\n",
        "        'metadata': {\n",
        "            'type': 'recipe',\n",
        "            'dish_name': data['dish_name'],\n",
        "            'source': source_file\n",
        "        }\n",
        "    }]\n",
        "\n",
        "\n",
        "def chunk_csv_simple(file_path):\n",
        "    \"\"\"\n",
        "    Put entire CSV in one chunk (for small CSVs ~5 lines)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Format as readable text\n",
        "        text_parts = [f\"Data from {file_path.split('/')[-1]}:\\n\"]\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            row_text = \" | \".join([f\"{col}: {val}\" for col, val in row.items() if pd.notna(val)])\n",
        "            text_parts.append(row_text)\n",
        "\n",
        "        text = \"\\n\".join(text_parts)\n",
        "\n",
        "        return [{\n",
        "            'text': text,\n",
        "            'metadata': {\n",
        "                'type': 'csv_data',\n",
        "                'source': file_path,\n",
        "                'filename': file_path.split('/')[-1],\n",
        "                'rows': len(df),\n",
        "                'columns': len(df.columns)\n",
        "            }\n",
        "        }]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error processing CSV {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def chunk_json_simple(file_path):\n",
        "    \"\"\"\n",
        "    Put each row of JSON in its own chunk - subject: text related to subject format\n",
        "    \"\"\"\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for key, val in data.items():\n",
        "                chunks.append({\n",
        "                    'text': f\"{key}: {val}\",\n",
        "                    'metadata': {\n",
        "                        'type': 'json_data',\n",
        "                        'source': file_path,\n",
        "                        'key': key,\n",
        "                        'value': val\n",
        "                    }\n",
        "                })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error processing JSON {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "1tJI0H_x4V0o"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== INGESTION =====\n",
        "\n",
        "def ingest_all_documents():\n",
        "    all_chunks = []\n",
        "\n",
        "    # Load core technique files\n",
        "    print(\"Loading core cooking technique files...\")\n",
        "    specific_files = {\n",
        "        '/content/drive/MyDrive/final-project/FCS.json': chunk_fcs_json,\n",
        "        '/content/drive/MyDrive/final-project/healthy_cooking_method.json': chunk_healthy_cooking,\n",
        "    }\n",
        "\n",
        "    for file_path, chunk_function in specific_files.items():\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            chunks = chunk_function(data, file_path)\n",
        "            all_chunks.extend(chunks)\n",
        "            print(f\"âœ“ Loaded {len(chunks)} chunks from {file_path.split('/')[-1]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error loading {file_path}: {e}\")\n",
        "\n",
        "    # Load all recipes from foc folder\n",
        "    print(\"\\nLoading recipes from foc folder...\")\n",
        "    foc_folder = '/content/drive/MyDrive/final-project/foc'\n",
        "    recipe_count = 0\n",
        "\n",
        "    if os.path.exists(foc_folder):\n",
        "        foc_json_files = glob.glob(os.path.join(foc_folder, '*.json'))\n",
        "\n",
        "        for file_path in foc_json_files:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                try:\n",
        "                    chunks = chunk_recipe_json(data, file_path)\n",
        "                    all_chunks.extend(chunks)\n",
        "                    recipe_count += 1\n",
        "                except:\n",
        "                    chunks = chunk_json_simple(file_path)\n",
        "                    all_chunks.extend(chunks)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  âœ— Error loading {file_path.split('/')[-1]}: {e}\")\n",
        "\n",
        "        print(f\"âœ“ Loaded {recipe_count} recipes\")\n",
        "    else:\n",
        "        print(f\"âœ— foc folder not found\")\n",
        "\n",
        "    # Load all CSV files (one chunk per file)\n",
        "    print(\"\\nLoading CSV files...\")\n",
        "    project_folder = '/content/drive/MyDrive/final-project'\n",
        "    csv_files = glob.glob(os.path.join(project_folder, '*.csv'))\n",
        "    csv_count = 0\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        csv_chunks = chunk_csv_simple(csv_file)\n",
        "        if csv_chunks:\n",
        "            all_chunks.extend(csv_chunks)\n",
        "            csv_count += 1\n",
        "            print(f\"âœ“ Loaded {csv_file.split('/')[-1]}\")\n",
        "\n",
        "    # Add all chunks to ChromaDB\n",
        "    if all_chunks:\n",
        "        documents = [chunk['text'] for chunk in all_chunks]\n",
        "        metadatas = [chunk['metadata'] for chunk in all_chunks]\n",
        "        ids = [f\"doc_{i}\" for i in range(len(all_chunks))]\n",
        "\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"âœ“ INGESTION COMPLETE\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Total chunks: {len(all_chunks)}\")\n",
        "        print(f\"  - Recipes: {recipe_count}\")\n",
        "        print(f\"  - CSV files: {csv_count}\")\n",
        "        print(f\"  - Other: {len(all_chunks) - recipe_count - csv_count}\")\n",
        "        print(f\"{'='*60}\")\n",
        "    else:\n",
        "        print(\"âœ— No chunks to ingest!\")\n",
        "\n",
        "    return len(all_chunks)"
      ],
      "metadata": {
        "id": "fxEZpQSS4ZLN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== QUERY & GENERATE =====\n",
        "\n",
        "def query_rag(question, n_results=3):\n",
        "    \"\"\"Retrieve relevant chunks\"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=n_results\n",
        "    )\n",
        "    return results['documents'][0], results['metadatas'][0]\n",
        "\n",
        "\n",
        "def ask_cooking_assistant(question):\n",
        "    \"\"\"Main RAG function with Hugging Face\"\"\"\n",
        "    # Retrieve relevant docs\n",
        "    contexts, metadatas = query_rag(question, n_results=3)\n",
        "    context_text = \"\\n\\n---\\n\\n\".join(contexts)\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are a helpful cooking assistant. Answer questions based on the provided context. Be concise and accurate.<|end|>\n",
        "<|user|>\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "Question: {question}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=300,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        return_full_text=False\n",
        "    )\n",
        "\n",
        "    answer = result[0]['generated_text'].strip()\n",
        "    sources = list(set([m['source'].split('/')[-1] for m in metadatas]))\n",
        "\n",
        "    return answer, sources"
      ],
      "metadata": {
        "id": "RlVdlBEl4deN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Ingest all documents\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COOKING ASSISTANT RAG SYSTEM - HUGGING FACE\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(\"Step 1: Ingesting documents...\")\n",
        "    total_chunks = ingest_all_documents()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"System Ready! Ask cooking questions.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Example queries\n",
        "    test_questions = [\n",
        "        \"How do I properly sautÃ© vegetables?\",\n",
        "        \"What's the difference between braising and stewing?\",\n",
        "        \"What ingredients do I need for chocolate chip cookies?\",\n",
        "        \"What are some healthy cooking methods?\",\n",
        "        \"Tell me about steaming as a cooking technique\",\n",
        "        \"Tell me about cooking technique that I need to know to make friend rice\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        answer, sources = ask_cooking_assistant(question)\n",
        "\n",
        "        print(f\"\\nA: {answer}\")\n",
        "        print(f\"\\nğŸ“š Sources: {', '.join(sources)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAF4hGrA4gEd",
        "outputId": "52f67572-ce47-4114-eace-e32a155da219"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "COOKING ASSISTANT RAG SYSTEM - HUGGING FACE\n",
            "============================================================\n",
            "\n",
            "Step 1: Ingesting documents...\n",
            "Loading core cooking technique files...\n",
            "âœ“ Loaded 32 chunks from FCS.json\n",
            "âœ“ Loaded 15 chunks from healthy_cooking_method.json\n",
            "\n",
            "Loading recipes from foc folder...\n",
            "âœ“ Loaded 33 recipes\n",
            "\n",
            "Loading CSV files...\n",
            "âœ“ Loaded cooking_methods.csv\n",
            "\n",
            "============================================================\n",
            "âœ“ INGESTION COMPLETE\n",
            "============================================================\n",
            "Total chunks: 239\n",
            "  - Recipes: 33\n",
            "  - CSV files: 1\n",
            "  - Other: 205\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "System Ready! Ask cooking questions.\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "Q: How do I properly sautÃ© vegetables?\n",
            "============================================================\n",
            "\n",
            "A: Unnamed: 0: 17\n",
            "COOKING METHODS: SautÃ©ing\n",
            "DESCRIPTION: SautÃ©ing is a method in which food is lightly tossed in little oil just enough to cover the base of the pan. The pan is covered with a lid and the flame or intensity of heat is reduced. The product obtained is slightly moist and tender but without any liquid or gravy. Foods cooked by sauteing are generally vegetables. MERITS: Takes less time. Simple technique. Minimum oil is used. DEMERITS: Scorching or burning is possible if not watched closely. Type of method: Dry heat methods. Healthy Cooking Method: SautÃ©\n",
            "\n",
            "ğŸ“š Sources: cooking_methods.csv, healthy_cooking_method.json\n",
            "\n",
            "============================================================\n",
            "Q: What's the difference between braising and stewing?\n",
            "============================================================\n",
            "\n",
            "A: The main differences between braising and stewing are:\n",
            "- Braising uses heat from an oven or stove to cook food slowly with some liquid, while stewing does not.\n",
            "- Braised dishes tend to be tougher than those that have been boiled or steamed due to the slow cooking process.\n",
            "- Stews can be made with a variety of ingredients, while braises are typically made with just meat or vegetables.\n",
            "\n",
            "ğŸ“š Sources: cooking_methods.csv, healthy_cooking_method.json\n",
            "\n",
            "============================================================\n",
            "Q: What ingredients do I need for chocolate chip cookies?\n",
            "============================================================\n",
            "\n",
            "A: Sugar, vanilla essence, brown sugar, baking soda, salt, chocolate chip, butter, dark chocolate chunks\n",
            "\n",
            "ğŸ“š Sources: foc_ingredients_Chocolate_chip_cookies.json, foc_ingredients_Chocolate_cupcakes.json\n",
            "\n",
            "============================================================\n",
            "Q: What are some healthy cooking methods?\n",
            "============================================================\n",
            "\n",
            "A: Healthy cooking methods include sautÃ©, boil, and moist heat.\n",
            "\n",
            "ğŸ“š Sources: healthy_cooking_method.json, FCS.json\n",
            "\n",
            "============================================================\n",
            "Q: Tell me about steaming as a cooking technique\n",
            "============================================================\n",
            "\n",
            "A: Steaming is a cooking method that uses boiling water to gently cook food without directly touching it. It is a low-fat and high-nutrition alternative to other cooking methods such as frying, roasting, grilling, and baking. It provides an evenly cooked and tender result without overcooking or undercooking, making it suitable for a variety of foods. Additionally, it does not require any additional seasoning or cooking oil, which makes it healthier than traditional cooking methods.\n",
            "\n",
            "ğŸ“š Sources: cooking_methods.csv, healthy_cooking_method.json, FCS.json\n",
            "\n",
            "============================================================\n",
            "Q: Tell me about cooking technique that I need to know to make friend rice\n",
            "============================================================\n",
            "\n",
            "A: The recipe for Korean Fried Rice calls for soy sauce, ginger, gouchujang, garlic cloves, sesame oil, boiled rice, kimchi, egg, carrot (jillion cut), peanut oil, peas, bacon, green shallots, and egg in a pan over medium heat with rice flour as a binding agent. To make Friend Rice, you will need boiled rice, spring onions, soy sauce, schezwan sauce, rice vinegar, eggs, chopped garlic, chopped carrots, crushed black pepper, diced bell peppers, salt, cabbage, and Korean rice cakes.\n",
            "\n",
            "ğŸ“š Sources: foc_ingredients_Korean_fried_rice.json, foc_ingredients_Korean_rice_cake.json, foc_ingredients_Egg_fried_rice.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what kind of sauce is make with emulsion?\"\n",
        "\n",
        "answer, sources = ask_cooking_assistant(question)\n",
        "\n",
        "print(f\"\\nA: {answer}\")\n",
        "print(f\"\\nğŸ“š Sources: {', '.join(sources)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nk-h1TVHR4N",
        "outputId": "262e2303-51c7-4d86-9545-88e670807e94"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A: Answer: Espagnole sauce, which is a cold emulsion sauce made using carrot, beef stock, chopped onion, tomato puree, garlic, celery, butter, black pepper, flour, bay leaves, and prepared in a pan over low heat for 15 minutes until thickened.\n",
            "\n",
            "ğŸ“š Sources: foc_ingredients_Espagnole_sauce.json, foc_ingredients_Oriental_sauce.json, foc_sections.json\n"
          ]
        }
      ]
    }
  ]
}