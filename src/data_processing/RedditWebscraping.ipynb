{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bba7c3-f2a5-4752-b79b-6c5849947863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "reddit = praw.Reddit(client_id=\"JT_iiB-NFwFyoknGwv5fYA\",client_secret=\"aVY6bitDc5BS8j4LX1cWusWjlgCaVQ\",user_agent=\"mac:eecs6893Project:v1.0 (by u/Superb-Cap8523)\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee43825-4a9c-4756-bfd0-a00bc0412076",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit= reddit.subreddit('AskCulinary')\n",
    "hot_askcul = subreddit.hot(2)\n",
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbae971-efc8-4444-a7ea-81cd537f74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"AskCulinary\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Choose what you want to scrape\n",
    "for post in subreddit.top(limit=1000):   # could also use .hot(), .new(), .search()\n",
    "    post.comments.replace_more(limit=None)  # get ALL comments\n",
    "    \n",
    "    comments_list = []\n",
    "    for c in post.comments.list():\n",
    "        comments_list.append({\n",
    "            \"author\": str(c.author),\n",
    "            \"body\": c.body,\n",
    "        })\n",
    "    \n",
    "    post_info = {\n",
    "        \"title\": post.title,\n",
    "        \"selftext\": post.selftext,\n",
    "        \"url\": post.url,\n",
    "        \"num_comments\": post.num_comments,\n",
    "        \"comments\": comments_list\n",
    "    }\n",
    "    \n",
    "    data.append(post_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2bf0f39-a7cc-49f0-b31a-375e19863887",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/Suvethika/Downloads/askculinary_data(limit=1000).json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367b20a-8b15-4a94-8949-b0c9fe195a35",
   "metadata": {},
   "source": [
    "Another Reddit Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef09d164-373b-4b38-bca7-94a373162201",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"cookingforbeginners\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Choose what you want to scrape\n",
    "for post in subreddit.top(limit=1000):   # could also use .hot(), .new(), .search()\n",
    "    post.comments.replace_more(limit=None)  # get ALL comments\n",
    "    \n",
    "    comments_list = []\n",
    "    for c in post.comments.list():\n",
    "        comments_list.append({\n",
    "            \"author\": str(c.author),\n",
    "            \"body\": c.body,\n",
    "        })\n",
    "    \n",
    "    post_info = {\n",
    "        \"title\": post.title,\n",
    "        \"selftext\": post.selftext,\n",
    "        \"num_comments\": post.num_comments,\n",
    "        \"comments\": comments_list\n",
    "    }\n",
    "    \n",
    "    data.append(post_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fb630c-7ec5-4f03-b83a-1cb04052111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/Suvethika/Downloads/cookingforbeginners_data(limit=1000).json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f11e7f-497f-47af-9fb2-ca39ac2e2f5c",
   "metadata": {},
   "source": [
    "Another Reddit: Cooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f192ce-50e6-48aa-ac86-0ac8a02c7c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit = reddit.subreddit(\"Cooking\")\n",
    "\n",
    "# data = []\n",
    "# count = 0\n",
    "# # Choose what you want to scrape\n",
    "# for post in subreddit.top():   # could also use .hot(), .new(), .search()\n",
    "#     post.comments.replace_more(limit=1000)  # get ALL comments\n",
    "#     if (count//10)==0:\n",
    "#         print(\"progress\")\n",
    "#     comments_list = []\n",
    "#     for c in post.comments.list():\n",
    "#         comments_list.append({\n",
    "#             \"author\": str(c.author),\n",
    "#             \"body\": c.body,\n",
    "#         })\n",
    "    \n",
    "#     post_info = {\n",
    "#         \"title\": post.title,\n",
    "#         \"selftext\": post.selftext,\n",
    "#         \"num_comments\": post.num_comments,\n",
    "#         \"comments\": comments_list\n",
    "#     }\n",
    "#     count+=1\n",
    "#     data.append(post_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99563a5c-9740-48d6-be4b-5cf933ce37bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 10 posts...\n",
      "Scraped 20 posts...\n",
      "Scraped 30 posts...\n",
      "Scraped 40 posts...\n",
      "Scraped 50 posts...\n",
      "Scraped 60 posts...\n",
      "Scraped 70 posts...\n",
      "Scraped 80 posts...\n",
      "Scraped 90 posts...\n",
      "Scraped 100 posts...\n"
     ]
    }
   ],
   "source": [
    "# grabs only top comment per post\n",
    "\n",
    "subreddit = reddit.subreddit(\"Cooking\")\n",
    "\n",
    "data = []\n",
    "count = 0\n",
    "\n",
    "for post in subreddit.top():   # could also use .hot(), .new(), .search()\n",
    "    post.comments.replace_more(limit=0)  # DO NOT fetch deep threads\n",
    "\n",
    "    # get only the top comment\n",
    "    if len(post.comments) > 0:\n",
    "        top_comment = post.comments[0]\n",
    "        comments_list = [{\n",
    "            \"author\": str(top_comment.author),\n",
    "            \"body\": top_comment.body,\n",
    "        }]\n",
    "    else:\n",
    "        comments_list = []  # no comments\n",
    "\n",
    "    post_info = {\n",
    "        \"title\": post.title,\n",
    "        \"selftext\": post.selftext,\n",
    "        \"num_comments\": post.num_comments,\n",
    "        \"comments\": comments_list\n",
    "    }\n",
    "\n",
    "    data.append(post_info)\n",
    "    count += 1\n",
    "\n",
    "    if count % 10 == 0:\n",
    "        print(f\"Scraped {count} posts...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f642d3-d869-495a-8aae-7e2db30c03ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/Suvethika/Downloads/Cooking_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3014f5-e321-45ac-b939-eb8fae34c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_comment(reddit, subreddit_name, post_limit=1000):\n",
    "    \"\"\"\n",
    "    Scrape top posts from a subreddit.\n",
    "    For each post, return title, selftext, num_comments, and ONLY the top comment.\n",
    "    \"\"\"\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    data = []\n",
    "    count = 0\n",
    "\n",
    "    print(f\"Scraping r/{subreddit_name}...\")\n",
    "\n",
    "    for post in subreddit.top(limit=post_limit):\n",
    "        post.comments.replace_more(limit=0)  # do NOT fetch deep threads\n",
    "\n",
    "        # get the top comment if exists\n",
    "        if len(post.comments) > 0:\n",
    "            top_comment = post.comments[0]\n",
    "            comments_list = [{\n",
    "                \"author\": str(top_comment.author),\n",
    "                \"body\": top_comment.body,\n",
    "            }]\n",
    "        else:\n",
    "            comments_list = []\n",
    "\n",
    "        post_info = {\n",
    "            \"title\": post.title,\n",
    "            \"selftext\": post.selftext,\n",
    "            \"num_comments\": post.num_comments,\n",
    "            \"comments\": comments_list\n",
    "        }\n",
    "\n",
    "        data.append(post_info)\n",
    "        count += 1\n",
    "\n",
    "        # progress update\n",
    "        if count % 20 == 0:\n",
    "            print(f\"Scraped {count} posts...\")\n",
    "\n",
    "    print(f\"Finished scraping {count} posts from r/{subreddit_name}!\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7fe83cb-3c21-4808-b7f2-ecef5082e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping r/AskBaking...\n",
      "Scraped 20 posts...\n",
      "Scraped 40 posts...\n",
      "Scraped 60 posts...\n",
      "Scraped 80 posts...\n",
      "Scraped 100 posts...\n",
      "Scraped 120 posts...\n",
      "Scraped 140 posts...\n",
      "Scraped 160 posts...\n",
      "Scraped 180 posts...\n",
      "Scraped 200 posts...\n",
      "Scraped 220 posts...\n",
      "Scraped 240 posts...\n",
      "Finished scraping 250 posts from r/AskBaking!\n"
     ]
    }
   ],
   "source": [
    "data = scrape_top_comment(reddit, \"AskBaking\", post_limit=1000)\n",
    "with open(\"/Users/Suvethika/Downloads/AskBaking_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b0fe602-906d-427c-adfe-623f36501130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping r/AskCulinary...\n",
      "Scraped 20 posts...\n",
      "Scraped 40 posts...\n",
      "Scraped 60 posts...\n",
      "Scraped 80 posts...\n",
      "Scraped 100 posts...\n",
      "Scraped 120 posts...\n",
      "Scraped 140 posts...\n",
      "Scraped 160 posts...\n",
      "Scraped 180 posts...\n",
      "Scraped 200 posts...\n",
      "Scraped 220 posts...\n",
      "Scraped 240 posts...\n",
      "Finished scraping 251 posts from r/AskCulinary!\n"
     ]
    }
   ],
   "source": [
    "data = scrape_top_comment(reddit, \"AskCulinary\", post_limit=1000)\n",
    "with open(\"/Users/Suvethika/Downloads/AskCulinary_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5afd455-02ef-42c9-9db0-9591f00699bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping r/cookingforbeginners...\n",
      "Scraped 20 posts...\n",
      "Scraped 40 posts...\n",
      "Scraped 60 posts...\n",
      "Scraped 80 posts...\n",
      "Scraped 100 posts...\n",
      "Scraped 120 posts...\n",
      "Scraped 140 posts...\n",
      "Scraped 160 posts...\n",
      "Scraped 180 posts...\n",
      "Scraped 200 posts...\n",
      "Scraped 220 posts...\n",
      "Scraped 240 posts...\n",
      "Finished scraping 250 posts from r/cookingforbeginners!\n"
     ]
    }
   ],
   "source": [
    "data = scrape_top_comment(reddit, \"cookingforbeginners\", post_limit=1000)\n",
    "with open(\"/Users/Suvethika/Downloads/cookingforbeginners_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902ba561-0eeb-47bc-827d-d8f82cd4d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_search_query(reddit, query, subreddit=\"all\", limit=200):\n",
    "    print(f\"Searching '{query}' in r/{subreddit}...\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for post in reddit.subreddit(subreddit).search(query, limit=limit):\n",
    "        post.comments.replace_more(limit=0)\n",
    "\n",
    "        top_comment = post.comments[0] if len(post.comments) > 0 else None\n",
    "\n",
    "        data.append({\n",
    "            \"title\": post.title,\n",
    "            \"selftext\": post.selftext,\n",
    "            \"subreddit\": str(post.subreddit),\n",
    "            \"num_comments\": post.num_comments,\n",
    "            \"top_comment\": top_comment.body if top_comment else None\n",
    "        })\n",
    "\n",
    "    print(f\"Finished collecting {len(data)} posts.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9382ee22-03d6-4b19-9f25-b8c3daa57f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 'cooking hacks' in r/all...\n",
      "Finished collecting 200 posts.\n"
     ]
    }
   ],
   "source": [
    "data = scrape_search_query(reddit, \"cooking hacks\")\n",
    "with open(\"/Users/Suvethika/Downloads/cookinghacks_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf106ae-2f78-45a4-81e2-53990f25a31f",
   "metadata": {},
   "source": [
    "AskBaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a530a-f9ca-4420-ab0a-6f4c06f37647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit = reddit.subreddit(\"AskBaking\")\n",
    "\n",
    "# data = []\n",
    "\n",
    "# # Choose what you want to scrape\n",
    "# for post in subreddit.top():   # could also use .hot(), .new(), .search()\n",
    "#     post.comments.replace_more(limit=None)  # get ALL comments\n",
    "    \n",
    "#     comments_list = []\n",
    "#     for c in post.comments.list():\n",
    "#         comments_list.append({\n",
    "#             \"author\": str(c.author),\n",
    "#             \"body\": c.body,\n",
    "#         })\n",
    "    \n",
    "#     post_info = {\n",
    "#         \"title\": post.title,\n",
    "#         \"selftext\": post.selftext,\n",
    "#         \"num_comments\": post.num_comments,\n",
    "#         \"comments\": comments_list\n",
    "#     }\n",
    "    \n",
    "#     data.append(post_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329b174-62b5-48eb-b4a7-907075a30282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/Users/Suvethika/Downloads/AskBaking_data.json\", \"w\") as f:\n",
    "#     json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5531dde-8803-4220-833f-73d742883cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_subreddit(reddit, subreddit_name, post_limit=200, top_n_comments=2):\n",
    "#     \"\"\"\n",
    "#     Scrape top posts from a subreddit and return title, selftext,\n",
    "#     and top N comments per post.\n",
    "#     \"\"\"\n",
    "\n",
    "#     subreddit = reddit.subreddit(subreddit_name)\n",
    "#     data = []\n",
    "\n",
    "#     print(f\"Scraping r/{subreddit_name}...\")\n",
    "\n",
    "#     for i, post in enumerate(subreddit.top(limit=post_limit)):\n",
    "#         post.comments.replace_more(limit=0)  # don't fetch deep threads\n",
    "\n",
    "#         # get the top N comments (default = 2)\n",
    "#         top_comments = post.comments[:top_n_comments]\n",
    "\n",
    "#         comments_list = [\n",
    "#             {\n",
    "#                 \"author\": str(c.author),\n",
    "#                 \"body\": c.body\n",
    "#             }\n",
    "#             for c in top_comments\n",
    "#         ]\n",
    "\n",
    "#         post_info = {\n",
    "#             \"title\": post.title,\n",
    "#             \"selftext\": post.selftext,\n",
    "#             \"num_comments\": post.num_comments,\n",
    "#             \"comments\": comments_list\n",
    "#         }\n",
    "\n",
    "#         data.append(post_info)\n",
    "\n",
    "#         # progress every 10 posts\n",
    "#         if (i + 1) % 10 == 0:\n",
    "#             print(f\"Scraped {i + 1} posts...\")\n",
    "\n",
    "#     print(f\"Finished scraping {len(data)} posts from r/{subreddit_name}!\")\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b59d49-572c-4656-b4a7-0f0ea59aef49",
   "metadata": {},
   "source": [
    "Gathering Recipe CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b13c597c-4ffb-40d6-a913-e633dcb69d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "recipe = pd.read_csv(\"/Users/Suvethika/Downloads/archive (1)/RecipeNLG_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98eb48da-0403-420c-a759-e2c4ae6a721b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                title  \\\n",
       "0           0  No-Bake Nut Cookies   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "\n",
       "                                             link    source  \\\n",
       "0  www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
       "\n",
       "                                                 NER  \n",
       "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efd23c-cfcf-48b2-8193-66ab0a4770ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
